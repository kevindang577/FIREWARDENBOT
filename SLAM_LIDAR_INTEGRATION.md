# SLAM Mapping & LiDAR Integration Guide
# Fire Warden Bot Enhanced Navigation System

## üéØ Overview

Your Fire Warden Bot now has comprehensive SLAM (Simultaneous Localization and Mapping) and LiDAR integration. This system combines:

- **Real-time SLAM mapping** using SLAM Toolbox
- **Advanced LiDAR processing** with obstacle detection
- **Web UI integration** for remote monitoring
- **Autonomous navigation** with Nav2 stack

## üèóÔ∏è Architecture Components

### 1. **SLAM Mapping System**
```yaml
Location: src/bringup/launch/slam_mapping.launch.py
Features:
- SLAM Toolbox integration
- EKF localization fusion
- Map server for saving/loading
- Configurable mapping/localization modes
```

### 2. **Enhanced LiDAR Processor**
```python
Location: src/vision/vision/enhanced_lidar_processor.py
Features:
- Real-time obstacle detection
- Point cloud generation
- Scan quality analysis
- Statistical monitoring
```

### 3. **Web Interface Integration**
```python
Location: ui/ros-data-server.py (updated)
New Endpoints:
- /api/slam - SLAM map & pose data
- /api/mapping - Complete mapping data
```

## üöÄ Quick Start

### Option 1: Full System Launch
```bash
# 1. Launch simulation with drone
ros2 launch bringup sim_one_drone.launch.py

# 2. Launch SLAM mapping
ros2 launch bringup slam_mapping.launch.py

# 3. Launch enhanced LiDAR processing
./launch_slam_lidar.sh

# 4. Launch web interface
cd ui && ./launch.sh
```

### Option 2: Individual Components
```bash
# SLAM only
ros2 launch bringup slam_mapping.launch.py drone_name:=drone1

# LiDAR processing only
ros2 run vision enhanced_lidar_processor --ros-args -p drone_name:=drone1

# Navigation stack
ros2 launch bringup navigation_full.launch.py
```

## üì° Topics & Data Flow

### Input Topics (from simulation/robot):
- `/model/drone1/scan` - LiDAR laser scan data
- `/model/drone1/odometry` - Robot odometry
- `/model/drone1/imu` - IMU sensor data

### Output Topics (generated by system):
- `/map` - SLAM-generated occupancy grid map
- `/odom` - Filtered odometry from EKF
- `/lidar/obstacles` - Detected obstacles
- `/lidar/pointcloud` - 3D point cloud
- `/lidar/analysis` - Scan quality metrics

### Web API Endpoints:
- `GET /api/slam` - SLAM map and pose estimation
- `GET /api/mapping` - Complete mapping data with LiDAR
- `GET /api/sensors` - IMU and laser sensor data

## ‚öôÔ∏è Configuration Files

### SLAM Parameters (`config/slam_params.yaml`):
```yaml
Key Settings:
- resolution: 0.05m (map resolution)
- max_laser_range: 20.0m
- mode: mapping (or localization)
- scan_topic: /scan
```

### Navigation Parameters (`config/nav2_params.yaml`):
```yaml
Key Settings:
- global_frame: map
- robot_base_frame: base_link
- scan_topic: scan
```

### EKF Parameters (`config/robot_localization.yaml`):
```yaml
Sensor Fusion:
- Odometry + IMU fusion
- Output: filtered pose estimate
```

## üéÆ Usage Examples

### 1. **Real-time Mapping**
```bash
# Start mapping mode
ros2 launch bringup slam_mapping.launch.py mapping_mode:=true

# Drive the robot around to build map
ros2 run teleop_twist_keyboard teleop_twist_keyboard --ros-args --remap cmd_vel:=/model/drone1/cmd_vel

# Save the map
ros2 run nav2_map_server map_saver_cli -f my_map
```

### 2. **Localization Mode**
```bash
# Switch to localization with existing map
ros2 launch bringup slam_mapping.launch.py mapping_mode:=false

# Load existing map
ros2 run nav2_map_server map_server --ros-args -p yaml_filename:=my_map.yaml
```

### 3. **Obstacle Monitoring**
```bash
# Monitor obstacles in real-time
ros2 topic echo /lidar/obstacles

# View point cloud in RViz
rviz2 -d config/slam_visualization.rviz
```

### 4. **Web Interface Access**
```bash
# Start ROS bridge for web access
python3 ui/ros-data-server.py

# Access SLAM data via HTTP
curl http://localhost:8090/api/slam
curl http://localhost:8090/api/mapping
```

## üîß Integration with Existing Features

### **Leaf Detection + SLAM**
The system now combines:
- Leaf detection from camera feed
- SLAM mapping from LiDAR
- Precise localization for leaf position mapping

### **Coverage Planning + Mapping**
Your existing coverage nodes now benefit from:
- Accurate SLAM-based localization
- Real-time obstacle avoidance
- Improved path planning

### **UI Dashboard**
The web interface now displays:
- Real-time SLAM map
- LiDAR obstacle data
- Combined sensor visualization

## üêõ Troubleshooting

### **No Map Data**
```bash
# Check SLAM node
ros2 node list | grep slam_toolbox

# Check scan data
ros2 topic echo /model/drone1/scan --once

# Verify transforms
ros2 run tf2_tools view_frames
```

### **Poor Mapping Quality**
```bash
# Check scan quality
ros2 topic echo /lidar/analysis

# Adjust SLAM parameters in slam_params.yaml:
# - Increase min_laser_range if too noisy
# - Decrease resolution for faster processing
# - Adjust loop closing parameters
```

### **Localization Issues**
```bash
# Check EKF fusion
ros2 topic echo /odom

# Verify sensor inputs
ros2 topic hz /model/drone1/odometry
ros2 topic hz /model/drone1/imu
```

## üéØ Advanced Features

### **Multi-Robot SLAM**
Your system supports multiple drones:
```bash
# Launch with multiple drones
ros2 launch bringup slam_mapping.launch.py drone_name:=drone2

# Each drone maintains separate map namespace
```

### **Map Persistence**
```bash
# Auto-save maps periodically
# Configure in slam_params.yaml:
use_map_saver: true
map_update_interval: 2.0
```

### **Custom LiDAR Processing**
Modify `enhanced_lidar_processor.py` for:
- Custom obstacle detection algorithms
- Environmental analysis (vegetation density)
- Fire detection integration with thermal data

## üìä Performance Monitoring

The system provides real-time metrics:
- **Scan Quality**: Coverage percentage, valid points
- **Mapping Rate**: Map updates per second
- **Localization Accuracy**: Pose covariance
- **Obstacle Detection**: Count and distribution

Monitor via web interface or command line:
```bash
ros2 topic echo /lidar/analysis
ros2 topic echo /slam_toolbox/scan_match_status
```

## üîó Next Steps

1. **Enhanced Visualization**: Add RViz configurations for better map display
2. **Autonomous Mission Planning**: Integrate with existing coverage algorithms
3. **Multi-Sensor Fusion**: Add camera-based SLAM for texture mapping
4. **Cloud Integration**: Save maps to cloud storage for mission continuity

Your Fire Warden Bot now has professional-grade SLAM and LiDAR capabilities! üöÅüî•
